# -*- coding: utf-8 -*-
"""8. AI Dec Batch 28,29thJan (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XzSW5BB08G1TqFcHBlqFzVzEj-ijLTGv

# Sentiment Analysis using NLP and Classification Algorithm

Sentiment Analysis is a means to identify the view or emotion behind a situation.

It basically means to analyse and find the emotion or intent behind a piece of text or speech or any mode of communication

This burger has a very bad taste- negative review

I ordered this pizza today- neurtral sentiment/review

I love this cheese sandwich, its so deliious- positive review
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix,roc_curve
from sklearn.metrics import classification_report, plot_confusion_matrix

df_train= pd.read_csv("train.txt",delimiter=";",names=['text','label'])
df_val=pd.read_csv("val.txt",delimiter=';',names=['text','label'])

df=pd.concat([df_train, df_val])
df.reset_index(inplace=True, drop=True)

print("Shape of the dataframe: ",df.shape)
df.sample(5)

sns.countplot(df.label)

"""As we can see that, we have6 labels or targets in the dataset. But for the sake of simpicity, we will merge these
labels into two classe i.e. Positive Sentiment and Negative Sentiment

Positive Sentiment- joy, love, surprise

Negative Sentiment- anger, sadness, fear

Now we will create a custom encoder to convert categorical target labels to numerical form i.e. 0 and 1
"""

def custom_encoder(df):
    df.replace(to_replace="surprise",value=1,inplace=True)
    df.replace(to_replace="love",value=1,inplace=True)
    df.replace(to_replace="joy",value=1,inplace=True)
    df.replace(to_replace="fear",value=0,inplace=True)
    df.replace(to_replace="anger",value=0,inplace=True)
    df.replace(to_replace="sadness",value=0,inplace=True)

custom_encoder(df['label'])

sns.countplot(df.label)

"""Now we can see that our target has changed to 0 and 1 i.e. 0 for negative and 1 for positive
and the data is more or less in a balanced state.

First, we will iterate through each record and using a regular expression we will get rid of any
characters apart from alphabets.

Then we will convert the string to lowercase

Then we will check for stopwords in the data and et rid of them.

Stopwords are commonly used words in a sentence such as the, an etc that do not add any value.

Then we will perform lemmatization on each word to convert it into its root format.
"""

lm=WordNetLemmatizer()

def text_transformation(df_col):
    corpus=[]
    for item in df_col:
        new_item=re.sub('[^a-zA-Z]','',str(item))
        new_item=new_item.lower()
        new_item=new_item.split()
        new_item=[lm.lemmatize(word) for word in new_item if word not in set(stopwords.words('english'))]
        corpus.append(' '.join(str(x) for x in new_item))
    return corpus

corpus=text_transformation(df['text'])

cv=CountVectorizer(ngram_range=(1,2))
traindata=cv.fit_transform(corpus)
X=traindata
y=df.label

"""Now comes the ML model creation part and we will use Random Forest classifier"""

parameters={'max_features':('auto','sqrt'),
           'n_estimators':[10,15],
           'max_depth':[10,None],
           'min_samples_split':[5],
           'min_samples_leaf':[1],
           'bootstrap':[True]
           }

"""Now we will fit the data into grid search and view the best parameters using the best_params_ attribute"""

grid_search= GridSearchCV(RandomForestClassifier(), parameters, cv=5, return_train_score=True, n_jobs=-1)
grid_search.fit(X,y)
grid_search.best_params_

"""We can view all the models and their respective parameters, mean test score and rank as GridSearch CV"""

for i in range(8):
    print('Parameters: ',grid_search.cv_results_['params'][i])
    print('Mean test score: ',grid_search.cv_results_['mean_test_score'][i])
    print("Rank: ",grid_search.cv_results_['rank_test_score'])

rfc=RandomForestClassifier(max_features=grid_search.best_params_['max_features'],
                          max_depth=grid_search.best_params_['max_depth'],
                          n_estimators=grid_search.best_params_['n_estimators'],
                          min_samples_split=grid_search.best_params_['min_samples_split'],
                          min_samples_leaf=grid_search.best_params_['min_samples_leaf'],
                          bootstrap=grid_search.best_params_['bootstrap']
                          )
rfc.fit(X,y)

"""Test Data Transformation"""

test_df=pd.read_csv('test.txt',delimiter=';',names=['text','label'])

test_df.head()

X_test, y_test= test_df.text, test_df.label

"""encode the labels into two classes 0 and 1"""

test_df= custom_encoder(y_test)

"""preprocessing of text"""

test_corpus= text_transformation(X_test)

"""convert the text data into vectors"""

testdata=cv.transform(test_corpus)

"""predict the target"""

predictions= rfc.predict(testdata)

""" Model Evaluation
    
We will evaluate our model using various metrics such as accuracy score, precision score, recall score,
confusion matrix and create a ROC curve to visualise how our model performed.
"""

acc_score=accuracy_score(y_test, predictions)
pre_score= precision_score(y_test, predictions)
rec_score=recall_score(y_test, predictions)

print('Accuracy Score: ',acc_score)
print('Precision Score:',pre_score)
print('Recall Score: ',rec_score)
print("-"*50)
cr=classification_report(y_test, predictions)
print(cr)

"""ROC Curve

We will plot the probability of the class using the predict_proba() method of random forest classsifier
and then we will plot the curve
"""

predictions_probability= rfc.predict_proba(testdata)
fpr,tpr,thresholds=roc_curve(y_test, predictions_probability[:,1])
plt.plot(fpr,tpr)
plt.plot([0,1])
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Posotive Rate')
plt.show()

"""Now we will check for custom input as well and let our model identify the sentiment of the input statement"""

def expression_check(prediction_input):
    if prediction_input==0:
        print("Input statement has negative sentiment")
    elif prediction_input==1:
        print("Input statement has positive sentiment")
    else:
        print("invalid statement")

"""function to take the input statement and perform the same transformation as we did earlier"""

def sentiment_predictor(input):
    input= text_transformation(input)
    transformed_input=cv.transform(input)
    prediction=rfc.predict(transformed_input)
    expression_check(prediction)

input1=["Sometimes I just don't want to go out"]
input2=["I bought a new phone and it's so good"]

sentiment_predictor(input1)
sentiment_predictor(input2)

"""# Movie Recommender System

Recommender Systems, also labelled as recommndation systems are statistical algorithms that recommend products
to users based on similarities between the buying trends of various user or similarities between
the products.

Collaborative Filtering

The process used to calculate similarities between the buying trends of various users or similarities between products
is called collaborative filtering

User Based Collaborative Filtering

If two user X and Y, like products A and B and there is another user Z who likes product A, then product B will
also be recommended to user Z


Item-based Collaborative Filtering

Products are recommended based on similarities between themselves. For instance, if a user likes product A
and Product A has properties X and Y, another product B with properties X and Y will also be recommended to the user.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""The dataset contains 100,000 movie reviews applied to 9000 movies by 600 users"""

movie_ids_titles= pd.read_csv("movies.csv")

movie_ids_titles.head()

movie_ids_titles.shape

movie_ids_ratings=pd.read_csv("ratings.csv")

movie_ids_ratings.head()

movie_ids_ratings.shape

"""Data Preprocessing- We need a data frame that consists of userid, movieid, title and ratings"""

movie_ids_titles.drop(["genres"],inplace=True,axis=1)

movie_ids_titles.head()

movie_ids_ratings.drop(["timestamp"],inplace=True, axis=1)

movie_ids_ratings.head()

merged_movie_df= pd.merge(movie_ids_ratings, movie_ids_titles, on='movieId')

merged_movie_df.head()

"""Data Visualisation- Let's first group the dataset by title and see what information we can get regarding the ratings of
movies
"""

merged_movie_df.groupby('title').describe()

merged_movie_df.groupby('title')['rating'].mean().head()

"""Let's sort the movie titles by descending order of the average user ratings"""

merged_movie_df.groupby('title')['rating'].mean().sort_values(ascending=False).head()

"""Let's now print the movies in the descending order of their rating counts"""

merged_movie_df.groupby('title')['rating'].count().sort_values(ascending=False).head()

"""A movie which is rated by large number of people is usually a good movie

Let's create a DataFrame that shows the title, mean rating and the rating counts
"""

movie_rating_mean_count=pd.DataFrame(columns=['rating_mean','rating_count'])

movie_rating_mean_count["rating_mean"]=merged_movie_df.groupby('title')['rating'].mean()

movie_rating_mean_count["rating_count"]=merged_movie_df.groupby('title')['rating'].count()

movie_rating_mean_count.head()

"""The above DataFrame contains movie title, average ratings (ratings_mean), and the number of rating_counts.

We will plot a histogram to see how the average ratings are distributed.
"""

plt.figure(figsize=(10,8))
sns.set_style("darkgrid")

movie_rating_mean_count['rating_mean'].hist(bins=30,color='purple')

"""In above grpah we can visualise that most of the movies have an average rating between 3 and 4

Distribution for rating counts
"""

plt.figure(figsize=(10,8))
sns.set_style("darkgrid")

movie_rating_mean_count["rating_count"].hist(bins=33,color="blue")

"""In above graph we can visualise that there are around 7,000 movies with less than 10 rating counts.

The number of movies decrease with an increase in ratings count.

Movies with more than 50 ratings are very few.

It is also interesting to see the relationship between mean ratings and rating counts of a movie.
"""

plt.figure(figsize=(10,8))

sns.set_style("darkgrid")

sns.regplot(x="rating_mean",y="rating_count", data= movie_rating_mean_count, color="brown")

"""From the above graph in the top right portion, you can see that the movies with a higher number of rating counts
tend to have higher mean ratings as well

Let's sort our dataset by rating counts and see the average ratings of the movies with the top 5 highest number fo ratings
"""

movie_rating_mean_count.sort_values("rating_count",ascending=False).head()

"""# Item Based Collaborative Filtering

In item based collaborative filtering, products are recommended based on common characteristics

The first step is to create a dataframe where each movie is represented by a column and rows contain
user ratings for movies.
"""

user_movie_rating_matrix= merged_movie_df.pivot_table(index="userId",columns="title",values="rating")

user_movie_rating_matrix

user_movie_rating_matrix.shape

"""The dataset contains 610 unique users and 9,719 unique movies

Now, we will find the movie recommendation based on a single movie and then based on multiple movies

Finding recommendation based on a single movie

Suppoese we want to find the recommendation based on the movie Pulp Fiction.

First we will filter the column that contains the user ratings for the movie.
"""

import warnings
warnings.filterwarnings("ignore")

pulp_fiction_ratings= user_movie_rating_matrix["Pulp Fiction (1994)"]

"""Next we will find the correlation between the user ratings of all the movies and the user ratings for the movie
pulp fiction
"""

pulp_fiction_correlations= pd.DataFrame(user_movie_rating_matrix.corrwith(pulp_fiction_ratings),columns=["pf_corr"])

pulp_fiction_correlations.sort_values("pf_corr",ascending=False).head(5)

"""Correlation itself is not giving meaningful results, one solution to this problem can be that in addition to the
correlation between the movies, we can also use rating counts, for the correlated movies as a criteria for
finding the best recommendation.
"""

pulp_fiction_correlations= pulp_fiction_correlations.join(movie_rating_mean_count["rating_count"])

pulp_fiction_correlations.head()

"""The pf_corr column contains some NaN values.

We will removeall the movies with null correlation
"""

pulp_fiction_correlations.dropna(inplace=False)

pulp_fiction_correlations.sort_values("pf_corr",ascending=False).head(5)

