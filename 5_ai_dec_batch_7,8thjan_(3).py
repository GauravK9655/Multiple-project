# -*- coding: utf-8 -*-
"""5. AI Dec Batch 7,8thJan (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zIL4DtJTbCHmf3OPYg2o42dFxv1f05sj

# Clustering

Clustering algorithms are unsupervised algorithms where the training data is not labeled.

Rather, the algorithm cluster or group the data sets based on common characteristics.

K-Means Clustering

Hierarchical Clustering

K-Means Clustering

K-Means clustering is one of the most commonly used algorithms for clustering unlabeled data.

In K-Means clustering, K refers to the number of clusters that you want your data to be grouped into.

In K-Means clustering, the number of clusters has to be defined before K clustering can be applied to the data points.

Steps for K-Means Clustering

1. Randomly assign cntroid values for each cluster

2. Calculate the euclidean distance between each data point and centroid values of all the clusters.

3. Assign the data point to the cluster of the centroid with the shortest distance.

4. Calculate and update centroid values based on the mean values of the coordinates of all
the data points of the corresponding cluster.

5. Repeat steps 2-4 until new centroid values for all the clusters are different from the previous centroid values.

# Customer Segmentation using K-Means Clustering
"""

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

dataset= pd.read_csv("Mall_Customers.csv")
dataset.head()

"""The spending score is the score assigned to customers based on their previous spending habits.

Customers with higher spending in the past have higher scores.
"""

dataset.shape

"""Histogram showing the annual income of the customers"""

sns.distplot(dataset['Annual Income (k$)'],kde=False,bins=50)

"""The output shows thatmost of the customers have incomes between 60 and 90K per year."""

sns.distplot(dataset['Spending Score (1-100)'],kde=False, bins=50, color="red")

"""The output shows that most of the customers have a spending score between 40 and 60"""

sns.regplot(x='Annual Income (k$)',y='Spending Score (1-100)',data=dataset)

sns.regplot(x="Age",y='Spending Score (1-100)',data=dataset)

"""The output shows an inverse linear relationship between age and spending score.

It can be inferred from the output that young people have higher spending compared to older people.
"""

dataset=dataset.filter(["Annual Income (k$)","Spending Score (1-100)"],axis=1)

dataset.head()

km_model=KMeans(n_clusters=4)
km_model.fit(dataset)

print(km_model.cluster_centers_)

print(km_model.labels_)

# printing the data points
plt.scatter(dataset.values[:,0],dataset.values[:,1],c=km_model.labels_,cmap='rainbow')

# print the centroids
plt.scatter(km_model.cluster_centers_[:,0],km_model.cluster_centers_[:,1],s=100,c='black')

"""To find the optimal number of customer segments, we need to find the optimal number of K because K defines the
number of clusters.

There is a way to find the ideal number of clusters. The method is known as the elbow method.

In the elbow method, the value of inertia obtained by training K-Means clusters with different numbers of K
is plotted on a graph.

The inertia represents the total distance between the data points within a cluster. Smaller inertia means that the
predicted clusters are robust and close to the actual clusters.
"""

# training K-Means on K values from 1 to 10
loss=[]
for i in range(1,11):
    km=KMeans(n_clusters=i).fit(dataset)
    loss.append(km.inertia_)

# printing loss against the number of clusters

import matplotlib.pyplot as plt
plt.plot(range(1,11),loss)
plt.title('Finding Optimal Clusters via Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('loss')
plt.show()

"""From the output, it can be seen that the value of inertia didn't decrease after five clusters.

Now we will segment our customers data into five groups by creating five clusters.
"""

# performning kmeans clustering using KMeans Class

km_model=KMeans(n_clusters=5)
km_model.fit(dataset)

# printing the data points
plt.scatter(dataset.values[:,0],dataset.values[:,1],c=km_model.labels_,cmap='rainbow')

# print the centroids
plt.scatter(km_model.cluster_centers_[:,0],km_model.cluster_centers_[:,1],s=100,c='black')

"""From the above output, you can see that the customers are divided into five segments.

The customers in the middle of the plot are the customers with average income and average spending.

The customers belonging to the purple clsuter are the ones with low income and low spending.

You need to target the customers who belong to the top right cluster. These are the customers with high incomes
and high spending in the past, and they are more likely to spend in the future as well. So, any new marketing campaigns
or advertisements should be targeted to these customers.

Finding Customers to target for Marketing
"""

# printing centroid values
print(km_model.cluster_centers_)

"""We have to fetch all the records from the cluster with id 1."""

cluster_map=pd.DataFrame()
cluster_map['data_index']=dataset.index.values
cluster_map['cluster']=km_model.labels_
cluster_map

cluster_map=cluster_map[cluster_map.cluster==1]
cluster_map.head()

cluster_map.info()

"""These are the customers who have high incomes and high spending and these customers should be targeted during
marketing campaigns.

# Dimensionality Reduction with PCA(Principal Component Analysis)

Dimensionality reduction refers to reducing the number of features in a dataset in such a way that the overall
performance of the algorithms trained on the dataset is minimally affected.

With dimensionality reduction, the training time of the statistical algorithm can significantly be reduced.

There are three main approaches used for dimensionality reduction-

1- Principal Component Analysis(PCA)

2- Linear Discriminant Analysis(LDA)

3-Singular Value Decomposition(SVD

# Principal Component Analysis(PCA)

PCA is an unsupervised dimensionality reduction technique that doesn't depend on the labels of a dataset.

Principal Component Analysis prioritizes features on the basis of their ability to cause maximum variance in the output.

The idea behind PCA is to capture those features that contain maximum variance about the dataset.

Advantages of PCA-

1- Correlated features can be detected and removed using PCA

2- It reduces overfitting because of a reduction in the number of features.

3- Model training can be expedited.
"""

import pandas as pd
import numpy as np
import seaborn as sns

iris_df=sns.load_dataset("iris")

iris_df.head()

X=iris_df.drop(['species'],axis=1)

y=iris_df['species']

# converting labels to numbers
from sklearn import preprocessing

le=preprocessing.LabelEncoder()
y=le.fit_transform(y)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.20, random_state=0)

# applying scaling on training and test data

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()

X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)

from sklearn.decomposition import PCA

pca=PCA()

X_train= pca.fit_transform(X_train)

X_test= pca.transform(X_test)

"""Once you have applied PCA on a dataset, you can use the explained_variance_ratio_ feature to print the variance caused
by all the features in the dataset.
"""

variance_ratios= pca.explained_variance_ratio_
print(variance_ratios)

"""The output above shows that 72.22 % of the variance in the dataset is casued by the first principal component,
while 23.97% of the variance is caused by the second principal component.
"""

from sklearn.decomposition import PCA

pca=PCA(n_components=2)
X_train= pca.fit_transform(X_train)
X_test= pca.transform(X_test)

from sklearn.linear_model import LogisticRegression

lg=LogisticRegression()
lg.fit(X_train, y_train)

y_pred=lg.predict(X_test)

from sklearn.metrics import accuracy_score

print(accuracy_score(y_test,y_pred))

"""The output shows that even with two features, the accuracy for correctly predicting the label for the iris plant is 86.66%

# K-Fold Cross Validation

K-Fold cross validation is a data splitting technique that lets you train and test the mdel on all subsets of the data.

With K-Fold Cross Validation, the data is divided into K parts. The experiments are also performed for K parts.

In each experiment, K-1 parts are used for training and the Kth part is used for testing.
"""

import pandas as pd
import numpy as np

wine_data= pd.read_csv("winequality-red.csv",sep=",")

wine_data.head()

X=wine_data.drop(['quality'],axis=1)

y=wine_data['quality']

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()
X=sc.fit_transform(X)

from sklearn.ensemble import RandomForestRegressor

rf_reg=RandomForestRegressor(random_state=42, n_estimators=500)

from sklearn.model_selection import cross_val_score

scores=cross_val_score(rf_reg, X,y,cv=5, scoring="neg_mean_absolute_error")

print(scores)

print("%0.2f accuracy with a standard deviation of %0.2f"%(scores.mean(),scores.std()))

"""# Deep Learning"""

# https://www.bing.com/images/search?view=detailV2&ccid=O0iwbD7U&id=4845535FA0CC6FDB641F56D51EC1E76AC515A340&thid=OIP.O0iwbD7ULb8KHCg2-qPy3gAAAA&mediaurl=https%3A%2F%2Fth.bing.com%2Fth%2Fid%2FR.3b48b06c3ed42dbf0a1c2836faa3f2de%3Frik%3DQKMVxWrnwR7VVg%26riu%3Dhttp%253a%252f%252fwhatsnext.nuance.com%252fwp-content%252fuploads%252fartificial-neural-network.png%26ehk%3DQnOcNgKLeonpoxX4Y99XcDExUrzY6ehxw35oXce1jHk%253d%26risl%3D%26pid%3DImgRaw%26r%3D0&exph=356&expw=296&q=neural+networks&simid=608046406034201783&form=IRPRST&ck=CB6FFFA2AF9C15FF127B7ED96860729F&selectedindex=0&ajaxhist=0&ajaxserp=0&pivotparams=insightsToken%3Dccid_kaxFfBnq*cp_9DBF928D8B4E0305140D8DBF1D787F34*mid_AD2EC9CA454FEBF1AA328BEBCEAB3C46A6BE94F3*simid_608034298438094607*thid_OIP.kaxFfBnq9MvXQQ!_b8alIxAHaI5&vt=0&sim=11&iss=VSI

# https://www.bing.com/images/search?view=detailV2&ccid=GWa2CNAh&id=3EB02540780073ECB247DB6C886F098D06EFAE3D&thid=OIP.GWa2CNAhABt0I1DIryl49QHaET&mediaurl=https%3a%2f%2fi.stack.imgur.com%2fUrm63.png&exph=396&expw=682&q=perceptron&simid=608051946460438379&FORM=IRPRST&ck=65C07BDE20175D7132A58145966CE2C5&selectedIndex=1&ajaxhist=0&ajaxserp=0

# https://www.bing.com/images/search?view=detailV2&ccid=eT%2fHzPAM&id=7F83E0B85CA8696759BD3417A4036A2CF5833EBA&thid=OIP.eT_HzPAM0gpsqjirjbR48AHaEm&mediaurl=https%3a%2f%2fmiro.medium.com%2fmax%2f2010%2f1*QhPZ4yGFGdXklywNlAk0Wg.png&exph=624&expw=1005&q=gradient+descent+in+machine+learning&simid=608027374971214203&FORM=IRPRST&ck=D45A4E0A4D8BCAD2A28292FD14ECA175&selectedIndex=3&ajaxhist=0&ajaxserp=0

"""Deep Learning is a field within Machine Learning that deals with building and using neural network models.

Neural Network Models mimic the functioning of a human brain. Neural Networks with more than three layers are
typically categorised as Deep Learning Networks.

The concept of Linear Regression forms a key foundation for Deep Learning projects.

Linear Regression is a linear model that explains the relationship between two or more variables.

Linear regression is usually used to predict continuous variables.

A related technique that is most used in Deep Learning is Logistic Regression.

Logistic Regression is a binary classification model that defines the relationship between two variables.

Perceptron=The perceptron is the unit for learning in an artificial neural networks. A perceptron represents the
algorithm for supervised learning in an ANN.

It resembles a human brain. Multiple inputs are fed into perceptron which in turn does computations and
outputs a boolean variable. It represents a single cell or node in a neural network.

In deep learning we replace slope of the model with weights called as w and intercept with bias called as b.

Weights and Biases become the parameters for a neural network. We then apply an activation function f that outputs
a boolean result based on the values.

The number of weights equals the number of inputs.

A ANN is a network of perceptrons. A Deep Neural Network usyally has three or more layers.

Each node has its own weights, biases and activation function. Each node is connected to all the nodes in the next layer
forming a dense network. The number of layers and the number of nodes in each layer are determined by experience
and trials and it changes from case to case.

The inputs or independent variables are sent from the input layer of the network. Data maybe preprocessed before using
them. Each node is a perceptron containing weights, bias and an activation function. The formula is applied on the input
and the outputs derived. As the process reaches the output layer the final prediction will be derived.

An ANN is created through a model training process. A neural network is represented by a set of parameters and
hyperparameters. Training an ANN means determining the right values for these parameters and hyperparameters such that
it maximises the accuracy of predictions for the given use case.

We use training data like regular ML, where we know both the dependent and independent variables.

We start the network architecture by intuition. We also initialise weights and biases to random values. Then we repeat
the iterations of applying weights and biases to the inputs and computing the error. Based on the error found we will
adjust the weights and biases to reduce the error.

We keep repeating the process of adjusting weights and biases until the error gets to an acceptable value. We will also
fine tune the network hyperparameters to improve the training speed and reduce iterations.

Finally, we will save the model as represented by its parameters and hyperparameters and then use it for predictions.

# Neural Netwok Architecture

Input Layer

A vector is an ordered list of numeric values. The input to Deep Learning model is usually a vector of numeric values.

Vectors are usually defined using NumPy arrays. It represents the feature variables or independent variables
that are used for prediction as well as training. Then preprocessing is done. Once the input data is ready
it can be passed to the deep learning model for training.

Hidden Layer

An artificial neural network can have one or more hidden layers. The more the number of hidden layers the deep
the network is. Each hidden layer can have one or more nodes.

Typically, the node count is configured in the range of 2^n. Examples mau be 8,16,32,54,128 etc.

A neural network is defined by the number of layers and nodes.

The output of each node in previous layer will become the input for every node in the current layer.

When there are more nodes and layers it usually results in better accuracy.

As a general practice, start with the small number and keep adding until an acceptable accuracy levels are obtained.

Weights and Biases

They form the basis for Deep Learning algorithms. Weights and biases are trainable parameters in a neural network model.

During a training process, the values for these weights and biases are determined such that they provide accurate
predictions.Weights and biases are nothing but a collection of numeric values. Each input for each node will have an
associated weight with it.

Activations Functions

An Activation Function plays an important role in creating the output of the node in the neural network.

An activation function takes the matrix output of the node and determines if and how the nodel will propagate
information to the next layer.

Activation functions act as filters to reduce noise and normalise the output. The main objective of activation function
is that it converts the output to a non-linear value.

They serve as a critical step in helping a neural network learn specific patterns in the data.

TanH- A TanH function normalizes the output in the range of -1 to +1

ReLu(Rectified Linear Unit)- A ReLu produces a zero if the output is negative. Else, it will reproduce the same input
verbatim.

Softmax Activation Function- A Softmax Activation function is used in the case of classification problems.
It produces a vector of probabilities for each of the possible classes in the outcomes. The sum of probabilities
will be equal to one. The class with the highest probability will be considered for prediction.

These all activation functions are added as hyperparameters in the model.

Output layer

The output layer is the final layer in the neural networkwhere desired predictions are obtained.

There is one output layer in a neural network that produces the desired final prediction. It has its own set of weights
and biases that are applied before the final output is derived.

The activation functoin for the output layer may be different than the hidden layers based on the problem.
For example- The softmax activation functions is used to derive final classes in a classification problem.

# Training a Neural Network Model

Set-up and Initialisation

In this process we apply a number of preprocessing techniques to convert samples in to numeric vectors.

To help with training, the input data is usually split into training, validation and test sets. Training dataset
is used for fitting the model parameters like weights and biases. Once, a model is trained the validation dataset
is used to check for its accuracy and error rates.

The results from the validation is then used to refine the model and recheck. When a final model is obtained,it is used
to predict on the test dataset to measure the final model performance.

The usual split of input data between the train, validation and test set is 80:10:10
        
In the beginning random initialisation is used to intialise the weights and biases to random values obtained
from a standard normal distribution whose mean is zero and standard deviation is one.

Forward Propagation

The input data is organised as samples and features. y is the actual value of the target in the training set and yhat will
be the value that will be predicted through forward propagation.

The forward propagation is exactly the same as doing actual prediction with the neural network.

For each sample, the inputs are sent through the designated neural network. For each node, we compute the outputs
based on the perceptron formula and pass them through the next layer. The final outcome yhat is then obtained at the end.

As we keep sending the sample to the neural network, we will collect yhat for each sample. This process is
repeated for all samples in the training dataset.

We will then compare the values of yhat and y and compute the error rates.
"""

