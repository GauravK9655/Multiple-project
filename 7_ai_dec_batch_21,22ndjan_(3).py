# -*- coding: utf-8 -*-
"""7. AI Dec Batch 21,22ndJan (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FZdm5l6WlgSOFG2z3Cfklwcby9qHAShH

# Spam/Ham Classification using Natural Language Processing

NLP- Natural Language Processing

NLP is a field concerned with the ability of a computer o understand, analyse, manipulate and
potentially generate human language.

Some of the use cases of NLP are Sentiment analysis, Topic Modelling, Text Classification

NLTK- Natural Language Toolkit- The NLTK is the mos utilised package for handling natural language processing tasks.
It is an open source library.
"""

import nltk

import pandas as pd
import numpy as np

dataset= pd.read_csv("SMSSpamCollection.tsv",sep="\t",header=None)

dataset.columns=['label','body_text']

dataset.head()

dataset['body_text'][0]

"""What is the shape of the data"""

print("Input data has {} rows and {} columns".format(len(dataset),len(dataset.columns)))

"""How many spam/ham are there?"""

print("Out of {} rows, {} are spam and {} are ham".format(len(dataset),len(dataset[dataset['label']=='spam']),
                                                         len(dataset[dataset['label']=="ham"])))

"""How much missing data is there?"""

print("Number of null values in label: {}".format(dataset['label'].isnull().sum()))

print("Number of nul values in text: {}".format(dataset['body_text'].isnull().sum()))

"""Preprocessing text data- Cleaning up text data is necessary to highlight attributes that you are going
to want your ML system to pick up.

Cleaning or preprocessing the data typically consists of a number of steps

1. Remove punctuation

2. Tokenization

3. Remove Sopwords

4. Lemmatization or Stemming
"""

import string
string.punctuation

def remove_punct(text):
    text_nopunct="".join([char for char in text if char not in string.punctuation])
    return text_nopunct

dataset['body_text_clean']=dataset['body_text'].apply(lambda x:remove_punct(x))

dataset.head()

"""Tokenization

Tokenization is splitting some string or sentence into a list of words.
"""

import re

def tokenize(text):
    tokens=re.split('\W',text)
    return tokens

dataset['body_text_tokenized']=dataset['body_text_clean'].apply(lambda x:tokenize(x.lower()))

dataset.head()

"""Stopwords

These are commonly used words like the, and, but, if that don't contribute much to the meaning of a sentence
"""

stopwords= nltk.corpus.stopwords.words('english')

def remove_stopwords(tokenized_list):
    text=[word for word in tokenized_list if word not in stopwords]
    return text

dataset['body_text_nostop']=dataset['body_text_tokenized'].apply(lambda x:remove_stopwords(x))

dataset.head()

"""Stemming

Stemming is the process of reducing inlected or derived words to their stem or root

More simply , the process of stemming means oftenly chopping off the end of a word to leave only the base.
"""

ps=nltk.PorterStemmer()

def stemming(tokenized_text):
    text=[ps.stem(word) for word in tokenized_text]
    return text

dataset['body_text_stemmed']=dataset['body_text_nostop'].apply(lambda x:stemming(x))

dataset.head()

"""Lemmatization

It is the process of grouping together the inflected forms of a word so they can be analysed as a single term,
identified by the word's lemma.


The lemma is the canonical form of a set of words, for instance type and typing would all be forms of the same lemma type.

It returns the dictionary form of theword. Example type, types and typing would all be simplified down to the word type
because that's the root word.
"""

wn= nltk.WordNetLemmatizer()

def lemmatizing(tokenized_text):
    text=[wn.lemmatize(word) for word in tokenized_text]
    return text

dataset['body_text_lemmatized']=dataset['body_text_nostop'].apply(lambda x:lemmatizing(x))

dataset.head()

"""Vectorization

Now that we have learned how to clean up text data we will be using to build the model. The process that we se to
convert text to a form that python and ML algorithms can understand is called vectorizing.

This is defined as the process of encoding text as integers to create feature vectors. A feature vector is an
n-dimensional vector of numerical features that represent some object.

So, in our context that means we will be taking individual text messages and converting it to a numeric vector
that represents that text message.

Count Vectorization

This creates a document-term matrix where the entry of each cell will be a count of the number of time
that word occured in that document.
"""

from sklearn.feature_extraction.text import CountVectorizer

def clean_text(text):
    text="".join([word.lower() for word in text if word not in string.punctuation])
    tokens=re.split('\W',text)
    text=[ps.stem(word) for word in tokens if word not in stopwords]
    return text

count_vect=CountVectorizer(analyzer=clean_text)
X_count=count_vect.fit_transform(dataset['body_text'])

print(X_count.shape)
print(count_vect.get_feature_names())

"""Apply count vectorizer to a smaller sample"""

data_sample= dataset[0:20]

count_vect_sample= CountVectorizer(analyzer=clean_text)
X_count_sample= count_vect_sample.fit_transform(data_sample['body_text'])

print(X_count_sample.shape)
print(count_vect_sample.get_feature_names())

"""Vectroizer Outputs Sparse Matrices

Sparse Matrix: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix
    will bestored by only storing the locations of the non-zero elements.
"""

X_count_sample

x_counts_df=pd.DataFrame(X_count_sample.toarray())
x_counts_df

x_counts_df.columns=count_vect_sample.get_feature_names_out()
x_counts_df

"""TF-IDF (Term Frequency, Inverse Document Frequency)

Creates a document term matrix where the column represents single unique terms (unigrams)
but the cell represents a weightage meant to reprersent how important a wor is to a document.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect=TfidfVectorizer(analyzer=clean_text)
X_tfidf= tfidf_vect.fit_transform(dataset['body_text'])

print(X_tfidf.shape)

print(tfidf_vect.get_feature_names())

"""Apply TfidfVectorizer to a smaller sample"""

data_sample=dataset[0:20]

tfidf_vect_sample=TfidfVectorizer(analyzer=clean_text)
X_tfidf_sample=tfidf_vect_sample.fit_transform(data_sample['body_text'])

print(X_tfidf_sample.shape)

print(tfidf_vect_sample.get_feature_names())

x_tfidf_df= pd.DataFrame(X_tfidf_sample.toarray())
x_tfidf_df.columns=tfidf_vect_sample.get_feature_names()
x_tfidf_df

"""# Feature Engineering: Feature Extraction"""

dataset=pd.read_csv("SMSSpamCollection.tsv",sep="\t",header=None)

dataset.columns=['label','body_text']

dataset.head()

"""create feature for text message length"""

dataset["body_len"]=dataset["body_text"].apply(lambda x:len(x)-x.count(" "))

dataset.head()

"""Create feature for % of text that is punctuation"""

def count_punct(text):
    count=sum([1 for char in text if char in string.punctuation])
    return round(count/(len(text)-text.count(" ")),3)*100

dataset['punct%']=dataset['body_text'].apply(lambda x:count_punct(x))

dataset.head()

import matplotlib.pyplot as plt
import numpy as np

bins=np.linspace(0,200,40)

plt.hist(dataset['body_len'],bins)
plt.title('Body Length Distribution')
plt.show()

bins=np.linspace(0,50,40)

plt.hist(dataset['punct%'],bins)
plt.title('Punctuation % Distribution')
plt.show()

import nltk
import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import string

stopwords= nltk.corpus.stopwords.words('english')

ps=nltk.PorterStemmer()

dataset=pd.read_csv("SMSSpamCollection.tsv",sep="\t",header=None)

dataset.columns=['label','body_text']

dataset.head()

def count_punct(text):
    count=sum([1 for char in text if char in string.punctuation])
    return round(count/(len(text)-text.count(" ")),3)*100

dataset["body_len"]=dataset["body_text"].apply(lambda x:len(x)-x.count(" "))
dataset['punct%']=dataset['body_text'].apply(lambda x:count_punct(x))
dataset.head()

def clean_text(text):
    text="".join([word.lower() for word in text if word not in string.punctuation])
    tokens=re.split('\W',text)
    text=[ps.stem(word) for word in tokens if word not in stopwords]
    return text

tfidf_vect=TfidfVectorizer(analyzer=clean_text)
X_tfidf= tfidf_vect.fit_transform(dataset['body_text'])

X_features=pd.concat([dataset['body_len'],dataset['punct%'],pd.DataFrame(X_tfidf.toarray())],axis=1)
X_features.head()

"""# Model using K-Fold Cross Validation"""

import warnings
warnings.filterwarnings("ignore")

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import KFold, cross_val_score

rf=RandomForestClassifier(n_jobs=-1)

k_fold=KFold(n_splits=5)

cross_val_score(rf, X_features, dataset['label'],cv=k_fold, scoring='accuracy',n_jobs=1)

"""# Model using Train Test Split"""

from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test= train_test_split(X_features, dataset['label'],test_size=0.3,random_state=0)

rf=RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)
rf_model=rf.fit(X_train, y_train)

sorted(zip(rf_model.feature_importances_,X_train.columns),reverse=True)[0:10]

y_pred=rf_model.predict(X_test)

precision, recall, fscore, support=score(y_test, y_pred, pos_label='spam',average='binary')

print('Precision {} / Recall {} / Accuracy {}'.format(round(precision,3),
                                                     round(recall,3),
                                                     round((y_pred==y_test).sum()/len(y_pred),3)))

"""# Sentiment Analysis using NLP and Classification Algorithm

Sentiment Analysis is a means to identify the view or emotion behind a situation.

It basically means to analyse and find the emotion or intent behind a piece of text or speech or any mode of communication

This burger has a very bad taste- negative review

I ordered this pizza today- neurtral sentiment/review

I love this cheese sandwich, its so deliious- positive review
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix,roc_curve
from sklearn.metrics import classification_report, plot_confusion_matrix

df_train= pd.read_csv("train.txt",delimiter=";",names=['text','label'])
df_val=pd.read_csv("val.txt",delimiter=';',names=['text','label'])

df=pd.concat([df_train, df_val])
df.reset_index(inplace=True, drop=True)

print("Shape of the dataframe: ",df.shape)
df.sample(5)

sns.countplot(df.label)

"""As we can see that, we have6 labels or targets in the dataset. But for the sake of simpicity, we will merge these
labels into two classe i.e. Positive Sentiment and Negative Sentiment

Positive Sentiment- joy, love, surprise

Negative Sentiment- anger, sadness, fear

Now we will create a custom encoder to convert categorical target labels to numerical form i.e. 0 and 1
"""

def custom_encoder(df):
    df.replace(to_replace="surprise",value=1,inplace=True)
    df.replace(to_replace="love",value=1,inplace=True)
    df.replace(to_replace="joy",value=1,inplace=True)
    df.replace(to_replace="fear",value=0,inplace=True)
    df.replace(to_replace="anger",value=0,inplace=True)
    df.replace(to_replace="sadness",value=0,inplace=True)

custom_encoder(df['label'])

sns.countplot(df.label)

"""Now we can see that our target has changed to 0 and 1 i.e. 0 for negative and 1 for positive
and the data is more or less in a balanced state.

First, we will iterate through each record and using a regular expression we will get rid of any
characters apart from alphabets.

Then we will convert the string to lowercase

Then we will check for stopwords in the data and et rid of them.

Stopwords are commonly used words in a sentence such as the, an etc that do not add any value.

Then we will perform lemmatization on each word to convert it into its root format.
"""

lm=WordNetLemmatizer()

def text_transformation(df_col):
    corpus=[]
    for item in df_col:
        new_item=re.sub('[^a-zA-Z]','',str(item))
        new_item=new_item.lower()
        new_item=new_item.split()
        new_item=[lm.lemmatize(word) for word in new_item if word not in set(stopwords.words('english'))]
        corpus.append(' '.join(str(x) for x in new_item))
    return corpus

corpus=text_transformation(df['text'])

cv=CountVectorizer(ngram_range=(1,2))
traindata=cv.fit_transform(corpus)
X=traindata
y=df.label

"""Now comes the ML model creation part and we will use Random Forest classifier"""

parameters={'max_features':('auto','sqrt'),
           'n_estimators':[10,15],
           'max_depth':[10,None],
           'min_samples_split':[5],
           'min_samples_leaf':[1],
           'bootstrap':[True]
           }

"""Now we will fit the data into grid search and view the best parameters using the best_params_ attribute"""

grid_search= GridSearchCV(RandomForestClassifier(), parameters, cv=5, return_train_score=True, n_jobs=-1)
grid_search.fit(X,y)
grid_search.best_params_

"""We can view all the models and their respective parameters, mean test score and rank as GridSearch CV"""

for i in range(8):
    print('Parameters: ',grid_search.cv_results_['params'][i])
    print('Mean test score: ',grid_search.cv_results_['mean_test_score'][i])
    print("Rank: ",grid_search.cv_results_['rank_test_score'])

rfc=RandomForestClassifier(max_features=grid_search.best_params_['max_features'],
                          max_depth=grid_search.best_params_['max_depth'],
                          n_estimators=grid_search.best_params_['n_estimators'],
                          min_samples_split=grid_search.best_params_['min_samples_split'],
                          min_samples_leaf=grid_search.best_params_['min_samples_leaf'],
                          bootstrap=grid_search.best_params_['bootstrap']
                          )
rfc.fit(X,y)

"""Test Data Transformation"""

test_df=pd.read_csv('test.txt',delimiter=';',names=['text','label'])

test_df.head()

