# -*- coding: utf-8 -*-
"""6. AI Dec Batch 14,15thJan (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MBZ3Nb3K_cs0wO2DUTgb3T7qExJu5ssF

# Deep Learning
"""

# https://www.bing.com/images/search?view=detailV2&ccid=O0iwbD7U&id=4845535FA0CC6FDB641F56D51EC1E76AC515A340&thid=OIP.O0iwbD7ULb8KHCg2-qPy3gAAAA&mediaurl=https%3A%2F%2Fth.bing.com%2Fth%2Fid%2FR.3b48b06c3ed42dbf0a1c2836faa3f2de%3Frik%3DQKMVxWrnwR7VVg%26riu%3Dhttp%253a%252f%252fwhatsnext.nuance.com%252fwp-content%252fuploads%252fartificial-neural-network.png%26ehk%3DQnOcNgKLeonpoxX4Y99XcDExUrzY6ehxw35oXce1jHk%253d%26risl%3D%26pid%3DImgRaw%26r%3D0&exph=356&expw=296&q=neural+networks&simid=608046406034201783&form=IRPRST&ck=CB6FFFA2AF9C15FF127B7ED96860729F&selectedindex=0&ajaxhist=0&ajaxserp=0&pivotparams=insightsToken%3Dccid_kaxFfBnq*cp_9DBF928D8B4E0305140D8DBF1D787F34*mid_AD2EC9CA454FEBF1AA328BEBCEAB3C46A6BE94F3*simid_608034298438094607*thid_OIP.kaxFfBnq9MvXQQ!_b8alIxAHaI5&vt=0&sim=11&iss=VSI

# https://www.bing.com/images/search?view=detailV2&ccid=GWa2CNAh&id=3EB02540780073ECB247DB6C886F098D06EFAE3D&thid=OIP.GWa2CNAhABt0I1DIryl49QHaET&mediaurl=https%3a%2f%2fi.stack.imgur.com%2fUrm63.png&exph=396&expw=682&q=perceptron&simid=608051946460438379&FORM=IRPRST&ck=65C07BDE20175D7132A58145966CE2C5&selectedIndex=1&ajaxhist=0&ajaxserp=0

# https://www.bing.com/images/search?view=detailV2&ccid=eT%2fHzPAM&id=7F83E0B85CA8696759BD3417A4036A2CF5833EBA&thid=OIP.eT_HzPAM0gpsqjirjbR48AHaEm&mediaurl=https%3a%2f%2fmiro.medium.com%2fmax%2f2010%2f1*QhPZ4yGFGdXklywNlAk0Wg.png&exph=624&expw=1005&q=gradient+descent+in+machine+learning&simid=608027374971214203&FORM=IRPRST&ck=D45A4E0A4D8BCAD2A28292FD14ECA175&selectedIndex=3&ajaxhist=0&ajaxserp=0

"""Deep Learning is a field within Machine Learning that deals with building and using neural network models.

Neural Network Models mimic the functioning of a human brain. Neural Networks with more than three layers are
typically categorised as Deep Learning Networks.

The concept of Linear Regression forms a key foundation for Deep Learning projects.

Linear Regression is a linear model that explains the relationship between two or more variables.

Linear regression is usually used to predict continuous variables.

A related technique that is most used in Deep Learning is Logistic Regression.

Logistic Regression is a binary classification model that defines the relationship between two variables.

Perceptron=The perceptron is the unit for learning in an artificial neural networks. A perceptron represents the
algorithm for supervised learning in an ANN.

It resembles a human brain. Multiple inputs are fed into perceptron which in turn does computations and
outputs a boolean variable. It represents a single cell or node in a neural network.

In deep learning we replace slope of the model with weights called as w and intercept with bias called as b.

Weights and Biases become the parameters for a neural network. We then apply an activation function f that outputs
a boolean result based on the values.

The number of weights equals the number of inputs.

A ANN is a network of perceptrons. A Deep Neural Network usyally has three or more layers.

Each node has its own weights, biases and activation function. Each node is connected to all the nodes in the next layer
forming a dense network. The number of layers and the number of nodes in each layer are determined by experience
and trials and it changes from case to case.

The inputs or independent variables are sent from the input layer of the network. Data maybe preprocessed before using
them. Each node is a perceptron containing weights, bias and an activation function. The formula is applied on the input
and the outputs derived. As the process reaches the output layer the final prediction will be derived.

An ANN is created through a model training process. A neural network is represented by a set of parameters and
hyperparameters. Training an ANN means determining the right values for these parameters and hyperparameters such that
it maximises the accuracy of predictions for the given use case.

We use training data like regular ML, where we know both the dependent and independent variables.

We start the network architecture by intuition. We also initialise weights and biases to random values. Then we repeat
the iterations of applying weights and biases to the inputs and computing the error. Based on the error found we will
adjust the weights and biases to reduce the error.

We keep repeating the process of adjusting weights and biases until the error gets to an acceptable value. We will also
fine tune the network hyperparameters to improve the training speed and reduce iterations.

Finally, we will save the model as represented by its parameters and hyperparameters and then use it for predictions.

# Neural Netwok Architecture

Input Layer

A vector is an ordered list of numeric values. The input to Deep Learning model is usually a vector of numeric values.

Vectors are usually defined using NumPy arrays. It represents the feature variables or independent variables
that are used for prediction as well as training. Then preprocessing is done. Once the input data is ready
it can be passed to the deep learning model for training.

Hidden Layer

An artificial neural network can have one or more hidden layers. The more the number of hidden layers the deep
the network is. Each hidden layer can have one or more nodes.

Typically, the node count is configured in the range of 2^n. Examples mau be 8,16,32,54,128 etc.

A neural network is defined by the number of layers and nodes.

The output of each node in previous layer will become the input for every node in the current layer.

When there are more nodes and layers it usually results in better accuracy.

As a general practice, start with the small number and keep adding until an acceptable accuracy levels are obtained.

Weights and Biases

They form the basis for Deep Learning algorithms. Weights and biases are trainable parameters in a neural network model.

During a training process, the values for these weights and biases are determined such that they provide accurate
predictions.Weights and biases are nothing but a collection of numeric values. Each input for each node will have an
associated weight with it.

Activations Functions

An Activation Function plays an important role in creating the output of the node in the neural network.

An activation function takes the matrix output of the node and determines if and how the nodel will propagate
information to the next layer.

Activation functions act as filters to reduce noise and normalise the output. The main objective of activation function
is that it converts the output to a non-linear value.

They serve as a critical step in helping a neural network learn specific patterns in the data.

TanH- A TanH function normalizes the output in the range of -1 to +1

ReLu(Rectified Linear Unit)- A ReLu produces a zero if the output is negative. Else, it will reproduce the same input
verbatim.

Softmax Activation Function- A Softmax Activation function is used in the case of classification problems.
It produces a vector of probabilities for each of the possible classes in the outcomes. The sum of probabilities
will be equal to one. The class with the highest probability will be considered for prediction.

These all activation functions are added as hyperparameters in the model.

Output layer

The output layer is the final layer in the neural networkwhere desired predictions are obtained.

There is one output layer in a neural network that produces the desired final prediction. It has its own set of weights
and biases that are applied before the final output is derived.

The activation functoin for the output layer may be different than the hidden layers based on the problem.
For example- The softmax activation functions is used to derive final classes in a classification problem.

# Training a Neural Network Model

Set-up and Initialisation

In this process we apply a number of preprocessing techniques to convert samples in to numeric vectors.

To help with training, the input data is usually split into training, validation and test sets. Training dataset
is used for fitting the model parameters like weights and biases. Once, a model is trained the validation dataset
is used to check for its accuracy and error rates.

The results from the validation is then used to refine the model and recheck. When a final model is obtained,it is used
to predict on the test dataset to measure the final model performance.

The usual split of input data between the train, validation and test set is 80:10:10
        
In the beginning random initialisation is used to intialise the weights and biases to random values obtained
from a standard normal distribution whose mean is zero and standard deviation is one.

Forward Propagation

The input data is organised as samples and features. y is the actual value of the target in the training set and yhat will
be the value that will be predicted through forward propagation.

The forward propagation is exactly the same as doing actual prediction with the neural network.

For each sample, the inputs are sent through the designated neural network. For each node, we compute the outputs
based on the perceptron formula and pass them through the next layer. The final outcome yhat is then obtained at the end.

As we keep sending the sample to the neural network, we will collect yhat for each sample. This process is
repeated for all samples in the training dataset.

We will then compare the values of yhat and y and compute the error rates.

Back Propagation

Once we have estimated the prediction error from forward propagation, we need to go back and adjust the weights
and biases.

Back propagation works in the reverse direction as the forward propagation.

In this we start from the output layer, we will then compute the delta value for this layer based on the error.

The delta is an adjustment that is applied to all the weights and biases in the layer. This results in new values for
weights and biases.

At the end of the backpropagation process we will have an updatedset of weights and biases that would reduce the overall
prediction error.

Gradient descent

Gradient Descent is the process of repeating forard and backward propagation in order to reduce error and move closer
to the desired model.

In gradient descent, we repeat the learning process of forward propagation, estimating error, backward propagation and then
adjusting the weights and biases.

As we do this process, the overall error estimated by the cost function will oscillate around and start moving closer
to zero.

Batches and Epochs

Batches and Epochs help control the number of passes through the neural network during the learning process.

A batch is a set of training examples, that are sent throught he neural network in one single pass.

The training data is divided into one or more batches. The neural network receives one batch at a time and does forward
propagation.

A training data is sent through the neural network multiple times duing the learning process. The total number of times
the entire dataset is sent through the neural network is called as Epoch.

Higher epoch sizes may lead to better accuracy but it also delays the learning process.

Validation & Testing

As we uild models, we also need to validate and test them against datasets to measure oout-of-sample error.

The model can be fine tuned and the learning process can be repeated based on the results against the validation
dataset.

After, all the fine tuning is completed and final model is obtained, the test data is used to evaluate or score
the mdoel. This is done only once at the end.

The evaluation results are then used to measure the performance of the model in terms of its final accuracy and
error rates.

# Deep Learning Example- Iris Dataset
"""

# Install related libraries for the project

# !pip install pandas
# !pip install tensorflow
# !pip install sklearn
# !pip install matplotlib

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import warnings
warnings.filterwarnings("ignore")

"""Data Preprocessing Steps:
    
1. Prepare input data for Deep learning

2. Load data into Pandas dataframe

3. Convert the dataframe into a numpy array

4. Scale the feature dataset

5. Use onehot encoding for the target variable

6. Split the dataset into training and test datasets

Load data and review content
"""

iris_data=pd.read_csv("iris.csv")
print(iris_data.head())

"""Use label encoder to convert String to numeric values for the target variable"""

from sklearn.preprocessing import LabelEncoder

label_encoder=LabelEncoder()
iris_data['Species']=label_encoder.fit_transform(iris_data['Species'])
print(iris_data.head())

"""Converting input to numpy array"""

np_iris=iris_data.to_numpy()
print(np_iris.shape)

"""Separate features and target variable"""

X_data=np_iris[:,0:4]
Y_data=np_iris[:,4]

print("\nFeatures before scaling: \n-----------------------------------------")
print(X_data[:5,:])
print("\nTarget before one-hot ecoding: \n------------------------------------")
print(Y_data[:5])

"""Create a standard scaler object that is fit on the input data"""

scaler=StandardScaler().fit(X_data)
X_data=scaler.transform(X_data)

"""Convert target variable as a one-hot encoded array"""

Y_data=tf.keras.utils.to_categorical(Y_data,3)

print("\nFeatures after scaling: \n--------------------------------------------------------")
print(X_data[:5,:])
print("\nTarget after one-hot encoding: \n------------------------------------------------")
print(Y_data[:5])

"""Splitting training and test data"""

X_train, X_test, Y_train, Y_test=train_test_split(X_data, Y_data, test_size=0.10)

print("\n Train Test Dimensions:\n--------------------------------")
print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

"""1. Create a model

2. Number of hiddern layers

3. Number of nodes in each layer

4. Activation functions

5. Loss function and accuracy measurements
"""

from tensorflow import keras

# Number of classes in target variable
NB_CLASSES=3

# create a sequential model in keras
model=tf.keras.models.Sequential()

# add the first hidden layer
model.add(keras.layers.Dense(128, #number of nodes
                            input_shape=(4,), # number of input variables
                            name="Hiddenlayer-1", # Logical name
                            activation='relu')) # activation function

# add a second hidden layer
model.add(keras.layers.Dense(128,
                            name="Hidden-Layer-2",
                            activation="relu"))

# add an ouput layer with softmax function
model.add(keras.layers.Dense(NB_CLASSES,
                           name="Output-Layer",
                           activation="softmax"))

# compile the model with loss and metrics
model.compile(loss="categorical_crossentropy",
             metrics=["accuracy"])

# print the model summary
model.summary()

"""Training and Evaluating the model"""

# Make it verbose so we can see the process
VERBOSE=1

# Set hyperparameters for training

# Set batch size
BATCH_SIZE=16
#Set the number of epochs
EPOCHS=20
#Set the validation split. 20% of the training dataset will be used for validation
VALIDATION_SPLIT=0.2

print("\nTraining Progress: \n--------------------------------------------------------------")

#Fitting the model- This will perform the entire training cycle, including forward propagation, loss computation,
#backward propagation and gradient descent. It is exeuted for the specified batch sizes and epochs.
# And performs validation after each epoch.

history= model.fit(X_train,
                  Y_train,
                  batch_size=BATCH_SIZE,
                  epochs=EPOCHS,
                  verbose=VERBOSE,
                  validation_split=VALIDATION_SPLIT)

print("\nAccuracy during training: \n----------------------------------------------------------------")

import matplotlib.pyplot as plt

# Plot the accuracy of the model after each epoch
pd.DataFrame(history.history)["accuracy"].plot(figsize=(8,5))
plt.title("Accuracy improvement with each epoch")
plt.show()

# Evaluate the model against the test dataset and print the result

print("\nEvaluate against test dataset:\n----------------------------------------------------------")

model.evaluate(X_test, Y_test)

# Saving a model
model.save("iris_save")

#load the model
loaded_model=keras.models.load_model("iris_save")

# print the model summary
loaded_model.summary()

# Predictions with Deep learning Model

# Raw prediction data

prediction_input=[[8.6,9.,12.4,6.4]]

# scale the prediction data with the same scaling object
scaled_input= scaler.transform(prediction_input)

# get the raw prediction probabilitis
raw_prediction=loaded_model.predict(scaled_input)
print("Raw Prediction Output (Probabilities):",raw_prediction)

# Find prediction
prediction=np.argmax(raw_prediction)
print("Prediction is",label_encoder.inverse_transform([prediction]))

"""# Convolutional Neural Network

38x38- 1,444

A Convolutional Neural Network (CNN) is a type of artificial neural network used in image recognition and
processing that is specifically designed to process pixel data

A CNN typically has three layers: a convolution layer, a pooling layer and a fully connected layer.
    
Convolution Layer: The Convolution Layer is the core building block of the CNN. This layer performs the dot product
between tow matrices, where one matrix is the set of input matrix and other matirx is kernel.

Pooling- The pooling layer is responsible for reducing the spatial size of the convolved feature

# CNN Model on MNIST Dataset for written digit classification

MNIST Dataset is the handwritten numbers taken as images.

All images are grey scale
"""

import pandas as pd
from keras.datasets import mnist
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense

import numpy as np
import matplotlib.pyplot as plt

"""Load the data"""

(X_train, y_train),(X_test,y_test)=mnist.load_data()

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""Understood the image format"""

X_train[0].shape

X_train[0]

plt.imshow(X_train[0], cmap="gray")

y_train[0]

"""Preprocess the image data"""

image_height, image_width=28,28

X_train= X_train.reshape(60000, image_height*image_width)
X_test=X_test.reshape(10000,image_height*image_width)

print(X_train.shape)
print(X_test.shape)

print(X_train[0])

X_train=X_train.astype('float32')
X_test=X_test.astype('float32')

X_train/=255.0
X_test/=255.0

print(X_train[0])

print(y_train.shape)
print(y_test.shape)

"""Converting the target value into 10 bins. So we will see that the output from a model will then
go into one of these bins.
"""

y_train= to_categorical(y_train,10)
y_test=to_categorical(y_test,10)

print(y_train.shape)
print(y_test.shape)

y_train[0]

"""Building the model using ANN"""

model=Sequential()

model.add(Dense(512, activation='relu',input_shape=(784,)))
model.add(Dense(512, activation='relu'))
model.add(Dense(10, activation='softmax'))

"""Compile the model"""

model.compile(optimizer="adam",loss='categorical_crossentropy',metrics=["accuracy"])
model.summary()

"""Total no of nodes * total number of weights +Total number of bias"""

784*512+512

512*512+512

512*10+10

"""Train the ANN model"""

history= model.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test))

plt.plot(history.history['accuracy'])

"""Evaluating the model"""

score= model.evaluate(X_test, y_test)

"""In neural networks we only had fully connected layer, otherwise known as dense layer.

With convolutional neural networks, we have more operations such as the convolution operation, max pooling , flattening
and also a fully connected or dense layer.

Importing the libraries
"""

from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.models import Sequential
from keras.datasets import mnist
from keras.utils.np_utils import to_categorical

(X_train, y_train),(X_test,y_test)=mnist.load_data()

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

X_train[0]

X_train=X_train.reshape(60000,28,28,1)
X_test=X_test.reshape(10000,28,28,1)

X_train=X_train.astype('float32')
X_test=X_test.astype('float32')

X_train/=255.0
X_test/=255.0

y_train=to_categorical(y_train,10)
y_test=to_categorical(y_test,10)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

#X_train[0]

"""CNN Model Development"""

cnn=Sequential()
cnn.add(Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1),padding='same',activation='relu'))
cnn.add(MaxPooling2D())
cnn.add(Conv2D(32, kernel_size=(3,3),padding='same',activation='relu'))
cnn.add(MaxPooling2D())
cnn.add(Flatten())
cnn.add(Dense(64, activation='relu'))
cnn.add(Dense(10,activation='softmax'))
cnn.compile(optimizer="adam",loss='categorical_crossentropy',metrics=["accuracy"])

print(cnn.summary())

history_cnn=cnn.fit(X_train, y_train, epochs=15, verbose=1, validation_data=(X_train, y_train))

plt.plot(history_cnn.history['accuracy'])

plt.plot(history_cnn.history['val_accuracy'])

"""# Image recognition using CNN on CIFAR-10 Dataset

In this project we will be using CIFAR-10 dataset. This dataset includes thousands of pictures of 10 different
kinds of objects like airplanes, automobiles, birds and so on.

Each image in the dataset includes a matching label so we know what kind of image it is.

The images in the CIFAR-10 dataset are only 32x32 pixels.
"""

import keras
from keras.datasets import cifar10
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from pathlib import Path
from keras.utils.np_utils import to_categorical

"""Load the dataset"""

(X_train, y_train),(X_test, y_test)=cifar10.load_data()

"""Normalize the data"""

X_train=X_train.astype('float32')
X_test=X_test.astype('float32')

X_train/=255.0
X_test/=255.0

"""Convert class vectors to binary class matrices"""

y_train=to_categorical(y_train,10)
y_test=to_categorical(y_test,10)

"""Dropout- The idea is that between certain layers, we will randomly drop/throw away some of the data
by cutting some of the connections between the layers. This is called as dropout.

Usually, we will add dropout right after max pooling layer or after a group of dense layers.

Model
"""

model=Sequential()
model.add(Conv2D(32,(3,3),padding='same',input_shape=(32,32,3),activation='relu'))
model.add(Conv2D(32,(3,3),activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))


model.add(Conv2D(64,(3,3),padding='same',activation='relu'))
model.add(Conv2D(32,(3,3),activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))


model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10,activation='softmax'))

"""Compile the model"""

model.compile(
loss='categorical_crossentropy',
optimizer='adam',
metrics=['accuracy'])

# print the model summary

model.summary()

"""Train the model"""

model.fit(
    X_train,
    y_train,
    batch_size=32,
    epochs=15,
    validation_data=(X_test,y_test),
    shuffle=True
)

# Save the neural network architecture

model_structure= model.to_json()
f=Path("model_structure.json")
f.write_text(model_structure)

# save the trained neural network weights
model.save_weights("model_weights.h5")

# Making predictions on the images

from keras.models import model_from_json
from pathlib import Path
from keras.preprocessing import image
import numpy as np

# CIFAR-10 Dataset class labels

class_labels=[
    "Planes",
    "Car",
    "Bird",
    "Cat",
    "Deer",
    "Dog",
    "Frog",
    "Horse",
    "Boat",
    "truck"
]

# load the json file that contains the model structure

f=Path("model_structure.json")
model_structure=f.read_text()

# recreeate the keras model object from the json data
model=model_from_json(model_structure)

# re-load the model training weights

model.load_weights("model_weights.h5")

# Load an image file to test
from tensorflow.keras.utils import load_img, img_to_array
img=load_img("dog.png",target_size=(32,32))
plt.imshow(img)

# Convert the image to a numpy array
from tensorflow.keras.utils import img_to_array
image_to_test=img_to_array(img)

# add a fourth dimension to the image, since keras expects a list of images, not a siingle image

list_of_images=np.expand_dims(image_to_test,axis=0)

# make predictions using the model

results=model.predict(list_of_images)

# since we are testing only ne image, we only need to check the first result

single_result=results[0]

# We will get a likelihood score for all 10 possible classes. Find out which class has the highest score

most_likely_class_index=int(np.argmax(single_result))
class_likelihood=single_result[most_likely_class_index]

#Get the name of the most likely class
class_label=class_labels[most_likely_class_index]

# Print tehe result

print("This is a image of  {}-Likelihood: {:2f}".format(class_label, class_likelihood))

